import streamlit as st
import pandas as pd
import ssl
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup
import io

@st.cache_data
def convert_df_to_excel(df):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    return output.getvalue()

def get_contents(urls, n):
    results = []
    progress_bar = st.progress(0, text="ìŠ¤í¬ë˜í•‘ ì§„í–‰ ì¤‘...")
    ctx = ssl._create_unverified_context()
    # ì¼ë¶€ ì›¹ì‚¬ì´íŠ¸ì˜ ì ‘ê·¼ ê±°ë¶€ë¥¼ í”¼í•˜ê¸° ìœ„í•œ í—¤ë” ì •ë³´
    headers = {'User-Agent': 'Mozilla/5.0'}

    for i, url in enumerate(urls[:n]):
        try:
            # í—¤ë”ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì²­(Request) ê°ì²´ë¥¼ ìƒì„±
            req = Request(url, headers=headers)
            webpage = urlopen(req, context=ctx)
            
            r_id = url[-16:]
            
            # íŒŒì´ì¬ ê¸°ë³¸ íŒŒì„œ 'html.parser' ì‚¬ìš©
            bsobj = BeautifulSoup(webpage.read(), 'html.parser')
            
            # idê°€ 'cont_view'ì¸ divë¥¼ ì°¾ìŒ
            content_div = bsobj.find('div', {'id': 'cont_view'})
            
            # divë¥¼ ì‹¤ì œë¡œ ì°¾ì•˜ëŠ”ì§€ í™•ì¸ (ê°€ì¥ ì¤‘ìš”í•œ ì•ˆì „ì¥ì¹˜)
            if content_div:
                text_content = content_div.get_text('\n', strip=True)
                results.append([r_id, text_content])
            else:
                st.warning(f"'{url}'ì—ì„œ ì½˜í…ì¸  ì˜ì—­('cont_view')ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ URLì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                
        except Exception as e:
            st.warning(f"'{url}' ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
        
        progress_bar.progress((i + 1) / n, text=f"ìŠ¤í¬ë˜í•‘ ì§„í–‰ ì¤‘... ({i+1}/{n})")
    
    progress_bar.empty()
    if not results:
        return pd.DataFrame() # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    return pd.DataFrame(results, columns=['r_id', 'content'])

def show():
    st.header("ìë£Œ ë¶ˆëŸ¬ì˜¤ê¸° ë° ìŠ¤í¬ë˜í•‘")
    st.info("ë…¼ì„¤ ì •ë³´ ì—‘ì…€ íŒŒì¼ê³¼ ê¸°ì‚¬ ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•˜ì„¸ìš”.")

    col1, col2 = st.columns(2)
    ron_info_df = None
    gisa_info_df = None

    with col1:
        excel_file = st.file_uploader("ë…¼ì„¤ ì •ë³´(`ron_info` ì‹œíŠ¸ í¬í•¨) ì—‘ì…€ íŒŒì¼", type=['xlsx', 'xls'], key="ws_excel_uploader")
        if excel_file:
            try:
                xls = pd.ExcelFile(excel_file)
                if 'ron_info' in xls.sheet_names:
                    ron_info_df = pd.read_excel(xls, sheet_name='ron_info')
                    st.dataframe(ron_info_df.head(2), use_container_width=True)
                else: 
                    st.error("'ron_info' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    with col2:
        gisa_file = st.file_uploader("ê¸°ì‚¬ ì •ë³´(`...txt`) íŒŒì¼", type="txt", key="ws_gisa_uploader")
        if gisa_file:
            try:
                gisa_info_df = pd.read_csv(gisa_file, sep='^', encoding='utf-8')
                st.dataframe(gisa_info_df.head(2), use_container_width=True)
            except Exception as e:
                st.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    if ron_info_df is not None:
        st.divider()
        st.subheader("ìŠ¤í¬ë˜í•‘ ì‹¤í–‰")
        urls = ron_info_df['url'].tolist()
        num_to_scrape = st.slider("ìŠ¤í¬ë˜í•‘í•  ë…¼ì„¤ ê°œìˆ˜", 1, len(urls), min(10, len(urls)), key="ws_slider")

        if st.button("ì›¹ ìŠ¤í¬ë˜í•‘ ë° ë°ì´í„° ê²°í•© ì‹œì‘", type="primary", key="ws_start_button"):
            if gisa_info_df is None:
                st.error("ê¸°ì‚¬ ì •ë³´(txt) íŒŒì¼ë„ í•¨ê»˜ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
            else:
                with st.spinner("ì‘ì—…ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                    contents_df = get_contents(urls, num_to_scrape)
                    
                    if not contents_df.empty and 'r_id_raw' in ron_info_df.columns:
                        r334_info1 = ron_info_df.drop('r_id', axis=1, errors='ignore')
                        combi_df = pd.merge(r334_info1, contents_df, left_on='r_id_raw', right_on='r_id', how='inner')
                        combi_df1 = combi_df[['r_id', 'r_id_raw', 'title', 'writer', 'gisa_class', 'date', 'url', 'year', 'content']]
                        st.session_state.scraped_df = combi_df1
                    elif contents_df.empty:
                        st.warning("ìŠ¤í¬ë˜í•‘ëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•´ì£¼ì„¸ìš”.")
                    else:
                        st.error("'ron_info_df'ì— 'r_id_raw' ì—´ì´ ì—†ìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    st.divider()

    if 'scraped_df' in st.session_state and st.session_state.scraped_df is not None:
        st.subheader("ì‘ì—… ê²°ê³¼")
        st.success("ìŠ¤í¬ë˜í•‘ ë° ê²°í•© ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.dataframe(st.session_state.scraped_df, use_container_width=True)
        
        excel_data = convert_df_to_excel(st.session_state.scraped_df)
        st.download_button(
            "ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
            excel_data,
            'scraped_data.xlsx',
            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            key="ws_download_button"
        )
        
        if st.button("ğŸ”„ ê²°ê³¼ ì§€ìš°ê³  ìƒˆë¡œ ì‹œì‘í•˜ê¸°", key="ws_reset_button"):
            if 'scraped_df' in st.session_state:
                del st.session_state.scraped_df
            st.rerun()

